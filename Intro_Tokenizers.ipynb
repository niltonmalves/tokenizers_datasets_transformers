{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_Tokenizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3QRpl7z4llKMWskUFx2LT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niltonmalves/tokenizers_datasets_transformers/blob/main/Intro_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/preprocessing"
      ],
      "metadata": {
        "id": "CfOoSWQaiOIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/tokenizers/python/latest/quicktour.html\n",
        "\n",
        "Quicktour \\\n",
        "**Build a tokenizer from scratch**"
      ],
      "metadata": {
        "id": "5RE8aSIebvlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCRseJCtbxTN",
        "outputId": "c64c2669-9e6d-4062-952a-49cbaf587bdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-14 13:15:18--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.244.94\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.244.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòwikitext-103-raw-v1.zip‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  77.5MB/s    in 2.4s    \n",
            "\n",
            "2022-03-14 13:15:21 (77.5 MB/s) - ‚Äòwikitext-103-raw-v1.zip‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the tokenizerÔÉÅ** \\\n",
        "In this tour, we will build and train [a Byte-Pair Encoding (BPE) tokenizer](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0#:~:text=%F0%9F%8F%83-,Byte%2DPair%20Encoding%20(BPE),Data%20Compression%E2%80%9D%20published%20in%201994.). For more information about the different type of tokenizers, check out this [guide](https://huggingface.co/transformers/tokenizer_summary.html) in the ü§ó Transformers documentation. Here, training the tokenizer means it will learn merge rules by:\n",
        "\n",
        "\n",
        "\n",
        "*   Start with all the characters present in the training corpus as tokens.\n",
        "*  Identify the most common pair of tokens and merge it into one token.\n",
        "*   Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.\n",
        "\n",
        "\n",
        "The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:"
      ],
      "metadata": {
        "id": "_FtWFi4AcRu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq81QIz2cnhd",
        "outputId": "77adb469-7fab-45bb-aa1b-1d18fe732c0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.5 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "cbmuL41hcm_I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To train our tokenizer on the wikitext files, we will need to instantiate a trainer, in this case a BpeTrainer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "\"\"\"\n",
        "We can set the training arguments like vocab_size or min_frequency (here left at their default values of 30,000 and 0) \\\n",
        "but the most important part is to give the special_tokens we plan to use later on (they are not used at all during training)\\\n",
        "so that they get inserted in the vocabulary.\n",
        "\n",
        "Note:\n",
        "The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[CLS]\" will get the ID 1 and so forth.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ES1dxhpZcsRh",
        "outputId": "78b84e46-5293-48f4-f69c-cc931312f02c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe can set the training arguments like vocab_size or min_frequency (here left at their default values of 30,000 and 0) but the most important part is to give the special_tokens we plan to use later on (they are not used at all during training)so that they get inserted in the vocabulary.\\n\\nNote:\\nThe order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[CLS]\" will get the ID 1 and so forth.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLS == classifier \\\n",
        "SEP == separator"
      ],
      "metadata": {
        "id": "S0fcdH0JifJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could train our tokenizer right now, but it wouldn‚Äôt be optimal. Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words: for instance we could get an \"it is\" token since those two words often appear next to each other. Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer. Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace."
      ],
      "metadata": {
        "id": "DzX7dQe6jh8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "1cgGFW_LjjFB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjEPgzvjjvHa",
        "outputId": "199d3236-c54c-4572-c279-9b0151fcbb92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  wikitext-103-raw  wikitext-103-raw-v1.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, we can just call the train() method with any list of files we want to use:\n",
        "files = [f\"./wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "QM7X9dCpjlBv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should only take a few seconds to train our tokenizer on the full wikitext dataset! To save the tokenizer in one file that contains all its configuration and vocabulary, just use the save() method:"
      ],
      "metadata": {
        "id": "DEPysJMUkqGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"./tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "ml99BjdEkrRz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer.from_file(\"./tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "6yy2MOW3k24p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the *tokenizer*"
      ],
      "metadata": {
        "id": "0zjiI50plGqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")"
      ],
      "metadata": {
        "id": "TAaVT_R_lJZZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4gy5BThlNHj",
        "outputId": "c3230a85-3f79-4070-c123-40cbbc913faf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ7ZyxgwlQAZ",
        "outputId": "a13953c0-cba7-4677-f361-a595bfc30316"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important feature of the ü§ó Tokenizers library is that it comes with full alignment tracking, meaning you can always get the part of your original sentence that corresponds to a given token. Those are stored in the offsets attribute of our Encoding object. For instance, let‚Äôs assume we would want to find back what caused the \"[UNK]\" token to appear, which is the token at index 9 in the list, we can just ask for the offset at the index:"
      ],
      "metadata": {
        "id": "cGdy4hb0mxpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.offsets[9])\n",
        "# (26, 27)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI8EggtYmzIU",
        "outputId": "d61eb876-81b2-4651-ef08-e6830791d94b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello, y'all! How are you üòÅ ?\"\n",
        "sentence[26:27]\n",
        "# \"üòÅ\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AzUek-06m01Z",
        "outputId": "0d487ca9-81b5-41db-e336-873320c0e586"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'üòÅ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Post-processing"
      ],
      "metadata": {
        "id": "KC0wnoU_nAnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We might want our tokenizer to automatically add special tokens, like \"[CLS]\" or \"[SEP]\". To do this, we use a post-processor. TemplateProcessing is the most commonly used, you just have to specify a template for the processing of single sentences and pairs of sentences, along with the special tokens and their IDs.\n",
        "\n",
        "When we built our tokenizer, we set \"[CLS]\" and \"[SEP]\" in positions 1 and 2 of our list of special tokens, so this should be their IDs. To double-check, we can use the token_to_id() method:"
      ],
      "metadata": {
        "id": "QnMuAZzCnDOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.token_to_id(\"[SEP]\")\n",
        "# 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD-g6nP8nDw7",
        "outputId": "cd68c726-a64f-491c-85d1-beb5210216ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we can set the post-processing to give us the traditional BERT inputs:"
      ],
      "metadata": {
        "id": "I0QN1NCxnI8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "zWK5FoU_nJVG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs go over this snippet of code in more details. First we specify the template for single sentences: those should have the form  \n",
        "```[CLS] $A [SEP] ```\n",
        " where $A represents our sentence.\n",
        "\n",
        "Then, we specify the template for sentence pairs, which should have the form ```[CLS] $A [SEP] $B [SEP]``` where ```$A ``` represents the first sentence and ```$B ``` the second one. The ```:1 ``` added in the template represent the type IDs we want for each part of our input: it defaults to 0 for everything (which is why we don‚Äôt have ```$A:0 ``` ) and here we set it to 1 for the tokens of the second sentence and the last ```[SEP]``` token.\n",
        "\n",
        "Lastly, we specify the special tokens we used and their IDs in our tokenizer‚Äôs vocabulary.\n",
        "\n",
        "To check out this worked properly, let‚Äôs try to encode the same sentence as before:\n"
      ],
      "metadata": {
        "id": "QEuCptNfnNZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output.tokens)\n",
        "# [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ODI87mnOC9",
        "outputId": "4fbf6b9b-ab62-4306-b231-c31922c92fbd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n",
        "print(output.tokens)\n",
        "# [\"[CLS]\", \"Hello\", \",\", \"y\", \"'\", \"all\", \"!\", \"[SEP]\", \"How\", \"are\", \"you\", \"[UNK]\", \"?\", \"[SEP]\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzNJLNxsokIp",
        "outputId": "49353dd9-cb26-4b41-b963-1f7bd95c90a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UpoUIVjWoyel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.type_ids)\n",
        "# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wictq4pomiX",
        "outputId": "829b7b92-d24e-4aa7-b0d1-0d827498932f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you save your tokenizer with save(), the post-processor will be saved along.\n",
        "output.save('out.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "QnWZoPnBoobm",
        "outputId": "b0ddd2be-503f-43e4-95d8-7a9d3535b4f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-961f2601621c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#If you save your tokenizer with save(), the post-processor will be saved along.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Encoding' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cl7E5jPuo6hh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}